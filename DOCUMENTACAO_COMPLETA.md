# ğŸ“š DocumentaÃ§Ã£o Completa - GeoAI Mentor

## ğŸ¯ Objetivo do Projeto

Desenvolver o "GeoAI Mentor", um chatbot assistente especializado que atua como mentor de carreira para geocientistas (geofÃ­sicos e geÃ³logos) que desejam migrar para a Ã¡rea de CiÃªncia de Dados e InteligÃªncia Artificial. O diferencial Ã© sua capacidade de manter o contexto da conversa, lembrando de perguntas e respostas anteriores para fornecer aconselhamento coeso e personalizado.

---

## ğŸ“¦ Tecnologias Utilizadas

- **Python 3.7+**
- **Google Gemini AI** (modelo: gemini-2.0-flash)
- **LangChain** - Framework para desenvolvimento de aplicaÃ§Ãµes com LLMs
- **python-dotenv** - Gerenciamento de variÃ¡veis de ambiente

---

## ğŸš€ EvoluÃ§Ã£o do Projeto

### **Etapa #1: ConexÃ£o BÃ¡sica com a API do Gemini**

#### ğŸ¯ Objetivo

Estabelecer a conexÃ£o com a API do Google Gemini e obter respostas simples do modelo de linguagem, sem qualquer tipo de memÃ³ria ou personalizaÃ§Ã£o.

#### ğŸ“ Requisitos Implementados

1. âœ… ImportaÃ§Ã£o de bibliotecas (`os`, `dotenv`, `ChatGoogleGenerativeAI`)
2. âœ… Carregamento de variÃ¡veis de ambiente do arquivo `.env`
3. âœ… InstanciaÃ§Ã£o do modelo Gemini com `gemini-2.0-flash` e `temperature=0.7`
4. âœ… CriaÃ§Ã£o de lista com duas perguntas sequenciais
5. âœ… Loop para enviar perguntas e imprimir respostas

#### ğŸ’» CÃ³digo da Etapa #1

```python
import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI

# Carrega as variÃ¡veis de ambiente do arquivo .env
load_dotenv()

# Instancia o modelo Gemini
modelo = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    temperature=0.7
)

# Lista de perguntas sequenciais
perguntas = [
    "Eu sou geofÃ­sico e quero migrar para a Ã¡rea de dados. Qual linguagem de programaÃ§Ã£o devo aprender primeiro?",
    "E que tipo de projeto de portfÃ³lio eu poderia criar usando essa linguagem?"
]

# Loop que envia cada pergunta para o modelo e imprime a resposta
for pergunta in perguntas:
    print(f"\nğŸ”µ Pergunta: {pergunta}")
    resposta = modelo.invoke(pergunta)
    print(f"\nğŸ¤– Resposta: {resposta.content}")
    print("-" * 80)
```

#### ğŸ“Š Resultado da Etapa #1

- âœ… ConexÃ£o bem-sucedida com a API do Gemini
- âœ… Respostas geradas para ambas as perguntas
- âš ï¸ **LimitaÃ§Ã£o**: O chatbot nÃ£o mantÃ©m contexto entre perguntas. Na segunda pergunta, ele nÃ£o lembra que estava falando sobre Python na primeira.

---

### **Etapa #2: Personalidade e Estrutura do Chatbot**

#### ğŸ¯ Objetivo

Dar personalidade ao chatbot atravÃ©s de um template de prompt estruturado, transformando-o no "GeoAI Mentor" especializado.

#### ğŸ“ Requisitos Implementados

1. âœ… ImportaÃ§Ã£o de `ChatPromptTemplate` e `StrOutputParser`
2. âœ… CriaÃ§Ã£o de template com trÃªs componentes:
   - Mensagem de sistema (personalidade)
   - Placeholder para histÃ³rico
   - Mensagem do usuÃ¡rio (query)
3. âœ… CriaÃ§Ã£o de cadeia (chain) usando LCEL (LangChain Expression Language)

#### ğŸ’» CÃ³digo da Etapa #2

```python
import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# Carrega as variÃ¡veis de ambiente do arquivo .env
load_dotenv()

# Instancia o modelo Gemini
modelo = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    temperature=0.7
)

# Cria o template de prompt com personalidade do GeoAI Mentor
template = ChatPromptTemplate.from_messages([
    ("system", "VocÃª Ã© o 'GeoAI Mentor', um assistente especializado em ajudar geocientistas a migrar para a Ã¡rea de CiÃªncia de Dados. Seja amigÃ¡vel e didÃ¡tico."),
    ("placeholder", "{historico}"),
    ("human", "{query}")
])

# Cria a cadeia (chain) usando LCEL
chain = template | modelo | StrOutputParser()

# Lista de perguntas sequenciais
perguntas = [
    "Eu sou geofÃ­sico e quero migrar para a Ã¡rea de dados. Qual linguagem de programaÃ§Ã£o devo aprender primeiro?",
    "E que tipo de projeto de portfÃ³lio eu poderia criar usando essa linguagem?"
]

# Loop que envia cada pergunta para o modelo e imprime a resposta
for pergunta in perguntas:
    print(f"\nğŸ”µ Pergunta: {pergunta}")
    # Usa a chain com histÃ³rico vazio (serÃ¡ implementado nas prÃ³ximas etapas)
    resposta = chain.invoke({"query": pergunta, "historico": []})
    print(f"\nğŸ¤– GeoAI Mentor: {resposta}")
    print("-" * 80)
```

#### ğŸ“Š Resultado da Etapa #2

- âœ… Chatbot com personalidade definida
- âœ… Respostas mais amigÃ¡veis e didÃ¡ticas
- âœ… Foco em geocientistas e exemplos especÃ­ficos da Ã¡rea
- âš ï¸ **LimitaÃ§Ã£o**: Ainda nÃ£o mantÃ©m contexto entre perguntas (histÃ³rico estÃ¡ vazio)

#### ğŸ” Conceitos LCEL (LangChain Expression Language)

O operador `|` (pipe) cria uma cadeia de processamento:

```python
chain = template | modelo | StrOutputParser()
```

**Fluxo de execuÃ§Ã£o:**

1. `template` â†’ Formata a entrada com o prompt estruturado
2. `modelo` â†’ Processa com o Gemini
3. `StrOutputParser()` â†’ Extrai apenas o texto da resposta

---

### **Etapa #3: MemÃ³ria Conversacional (Final)**

#### ğŸ¯ Objetivo

Implementar a capacidade de memÃ³ria conversacional, permitindo que o chatbot lembre de interaÃ§Ãµes anteriores e mantenha contexto ao longo da conversa.

#### ğŸ“ Requisitos Implementados

1. âœ… ImportaÃ§Ã£o de `InMemoryChatMessageHistory` e `RunnableWithMessageHistory`
2. âœ… CriaÃ§Ã£o de dicionÃ¡rio `memoria_sessoes` para armazenar histÃ³ricos
3. âœ… ImplementaÃ§Ã£o da funÃ§Ã£o `obter_historico_por_sessao()` com padrÃ£o singleton
4. âœ… CriaÃ§Ã£o de `cadeia_com_memoria` usando `RunnableWithMessageHistory`
5. âœ… ModificaÃ§Ã£o do loop para usar a cadeia com memÃ³ria e `session_id`

#### ğŸ’» CÃ³digo da Etapa #3 (Final)

```python
import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

# Carrega as variÃ¡veis de ambiente do arquivo .env
load_dotenv()

# Instancia o modelo Gemini
modelo = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    temperature=0.7
)

# Cria o template de prompt com personalidade do GeoAI Mentor
template = ChatPromptTemplate.from_messages([
    ("system", "VocÃª Ã© o 'GeoAI Mentor', um assistente especializado em ajudar geocientistas a migrar para a Ã¡rea de CiÃªncia de Dados. Seja amigÃ¡vel e didÃ¡tico."),
    ("placeholder", "{historico}"),
    ("human", "{query}")
])

# Cria a cadeia (chain) usando LCEL
chain = template | modelo | StrOutputParser()

# DicionÃ¡rio para armazenar o histÃ³rico de cada sessÃ£o
memoria_sessoes = {}

# FunÃ§Ã£o para obter ou criar histÃ³rico por sessÃ£o (padrÃ£o singleton)
def obter_historico_por_sessao(session_id: str):
    if session_id not in memoria_sessoes:
        memoria_sessoes[session_id] = InMemoryChatMessageHistory()
    return memoria_sessoes[session_id]

# Cria a cadeia com memÃ³ria
cadeia_com_memoria = RunnableWithMessageHistory(
    runnable=chain,
    get_session_history=obter_historico_por_sessao,
    input_messages_key="query",
    history_messages_key="historico"
)

# Lista de perguntas sequenciais
perguntas = [
    "Eu sou geofÃ­sico e quero migrar para a Ã¡rea de dados. Qual linguagem de programaÃ§Ã£o devo aprender primeiro?",
    "E que tipo de projeto de portfÃ³lio eu poderia criar usando essa linguagem?"
]

# ConfiguraÃ§Ã£o da sessÃ£o
config = {"configurable": {"session_id": "sessao_geocientista_01"}}

# Loop que envia cada pergunta para o modelo e imprime a resposta
for pergunta in perguntas:
    print(f"\nğŸ”µ Pergunta: {pergunta}")
    # Usa a cadeia com memÃ³ria
    resposta = cadeia_com_memoria.invoke({"query": pergunta}, config=config)
    print(f"\nğŸ¤– GeoAI Mentor: {resposta}")
    print("-" * 80)

# Exibe o histÃ³rico completo da conversa
print("\n" + "=" * 80)
print("ğŸ“ HISTÃ“RICO DA CONVERSA:")
print("=" * 80)
historico = obter_historico_por_sessao("sessao_geocientista_01")
for mensagem in historico.messages:
    tipo = "ğŸ‘¤ UsuÃ¡rio" if mensagem.type == "human" else "ğŸ¤– GeoAI Mentor"
    print(f"\n{tipo}: {mensagem.content[:100]}...")
print("=" * 80)
```

#### ğŸ“Š Resultado da Etapa #3

- âœ… Chatbot mantÃ©m contexto completo da conversa
- âœ… Segunda pergunta compreende que estÃ¡ falando sobre Python
- âœ… SugestÃµes de projetos especÃ­ficas para Python e geofÃ­sica
- âœ… HistÃ³rico da conversa armazenado e recuperÃ¡vel
- âœ… Suporte para mÃºltiplas sessÃµes independentes

#### ğŸ” Conceitos de MemÃ³ria

**PadrÃ£o Singleton:**

```python
def obter_historico_por_sessao(session_id: str):
    if session_id not in memoria_sessoes:
        memoria_sessoes[session_id] = InMemoryChatMessageHistory()
    return memoria_sessoes[session_id]
```

Garante que cada `session_id` tenha uma Ãºnica instÃ¢ncia de histÃ³rico.

**RunnableWithMessageHistory:**

- Envelopa a cadeia existente
- Injeta automaticamente o histÃ³rico no placeholder
- Salva novas mensagens apÃ³s cada interaÃ§Ã£o

---

## ğŸ“ˆ Comparativo das Etapas

| Aspecto               | Etapa #1 | Etapa #2 | Etapa #3 |
| --------------------- | -------- | -------- | -------- |
| **ConexÃ£o API**       | âœ…       | âœ…       | âœ…       |
| **Personalidade**     | âŒ       | âœ…       | âœ…       |
| **Estrutura LCEL**    | âŒ       | âœ…       | âœ…       |
| **MemÃ³ria**           | âŒ       | âŒ       | âœ…       |
| **Contexto**          | âŒ       | âŒ       | âœ…       |
| **MÃºltiplas SessÃµes** | âŒ       | âŒ       | âœ…       |

---

## ğŸ“ Conceitos Importantes Aprendidos

### 1. **LangChain Expression Language (LCEL)**

Sintaxe declarativa para criar cadeias de processamento usando o operador `|`:

```python
chain = template | modelo | StrOutputParser()
```

### 2. **Templates de Prompt**

Estruturam a conversa em trÃªs partes:

- **System**: Define personalidade e instruÃ§Ãµes
- **Placeholder**: EspaÃ§o para histÃ³rico dinÃ¢mico
- **Human**: Entrada do usuÃ¡rio

### 3. **Gerenciamento de MemÃ³ria**

- `InMemoryChatMessageHistory`: Armazena mensagens em memÃ³ria
- `RunnableWithMessageHistory`: Gerencia injeÃ§Ã£o automÃ¡tica de histÃ³rico
- `session_id`: Permite mÃºltiplas conversas independentes

### 4. **PadrÃ£o Singleton**

Garante uma Ãºnica instÃ¢ncia de histÃ³rico por sessÃ£o:

```python
if session_id not in memoria_sessoes:
    memoria_sessoes[session_id] = InMemoryChatMessageHistory()
return memoria_sessoes[session_id]
```

---

## ğŸ”§ Arquivos do Projeto

```
MyGeoAI_Mentor/
â”œâ”€â”€ .env                          # Chave de API do Google Gemini
â”œâ”€â”€ chatbot_mentor.py             # CÃ³digo principal (Etapa #3 final)
â”œâ”€â”€ requirements.txt              # DependÃªncias do projeto
â”œâ”€â”€ README.md                     # InstruÃ§Ãµes de uso
â”œâ”€â”€ DOCUMENTACAO_COMPLETA.md      # Este arquivo
â””â”€â”€ listar_modelos.py             # Script auxiliar para listar modelos
```

---

## ğŸ¯ ConclusÃ£o

O projeto **GeoAI Mentor** evoluiu atravÃ©s de trÃªs etapas bem definidas:

### âœ¨ Conquistas

1. **Etapa #1**: Estabelecemos conexÃ£o funcional com o Gemini e validamos a comunicaÃ§Ã£o bÃ¡sica
2. **Etapa #2**: Adicionamos personalidade e estrutura profissional ao chatbot
3. **Etapa #3**: Implementamos memÃ³ria conversacional completa

### ğŸš€ Resultado Final

Um chatbot especializado que:

- âœ… MantÃ©m contexto ao longo da conversa
- âœ… Fornece respostas personalizadas para geocientistas
- âœ… Ã‰ amigÃ¡vel e didÃ¡tico
- âœ… Suporta mÃºltiplas sessÃµes independentes
- âœ… Utiliza o modelo mais recente do Gemini (2.0-flash)

### ğŸ’¡ AplicaÃ§Ãµes PrÃ¡ticas

O GeoAI Mentor pode ser expandido para:

- Interface web com Streamlit ou Gradio
- Chatbot em plataformas de mensagem (WhatsApp, Telegram)
- Sistema de mentoria para empresas de geociÃªncias
- Plataforma educacional para cursos online
- Assistente de orientaÃ§Ã£o de carreira

### ğŸ“ Habilidades Desenvolvidas

- âœ… IntegraÃ§Ã£o com APIs de IA generativa (Google Gemini)
- âœ… Uso do framework LangChain
- âœ… Gerenciamento de memÃ³ria conversacional
- âœ… EstruturaÃ§Ã£o de prompts eficazes
- âœ… PadrÃµes de design (Singleton)
- âœ… Desenvolvimento de aplicaÃ§Ãµes com IA

### ğŸ”® PrÃ³ximos Passos Sugeridos

1. **Interface GrÃ¡fica**: Criar interface web com Streamlit
2. **PersistÃªncia**: Salvar histÃ³rico em banco de dados
3. **RAG (Retrieval-Augmented Generation)**: Adicionar base de conhecimento especÃ­fica
4. **Multi-agentes**: Criar agentes especializados em diferentes Ã¡reas
5. **AnÃ¡lise de Sentimento**: Adaptar respostas ao estado emocional do usuÃ¡rio
6. **Feedback Loop**: Implementar sistema de avaliaÃ§Ã£o das respostas
7. **Deploy**: Publicar na nuvem (AWS, Google Cloud, Azure)

---

## ğŸ“š Recursos Adicionais

### DocumentaÃ§Ã£o Oficial

- [Google Gemini AI](https://ai.google.dev/)
- [LangChain Documentation](https://python.langchain.com/)
- [LangChain Google GenAI](https://python.langchain.com/docs/integrations/llms/google_ai)

### Tutoriais Recomendados

- [LangChain Quickstart](https://python.langchain.com/docs/get_started/quickstart)
- [Building Chatbots with Memory](https://python.langchain.com/docs/expression_language/how_to/message_history)
- [Prompt Engineering Guide](https://www.promptingguide.ai/)

---

## ğŸ‘¨â€ğŸ’» Autor

Projeto desenvolvido como parte do desafio de criaÃ§Ã£o de chatbot especializado para geocientistas.

**Data de ConclusÃ£o**: 02 de Dezembro de 2025

---

## âœ… Pontos de RevisÃ£o e ValidaÃ§Ã£o

Ao concluir o projeto, Ã© importante verificar se nÃ£o cometeu algum destes erros comuns:

### ğŸ”‘ 1. Chave de API

**âš ï¸ AtenÃ§Ã£o**: Como estamos usando **Google Gemini** (nÃ£o OpenAI), a configuraÃ§Ã£o Ã© diferente:

- âœ… **Correto para Gemini**: `GOOGLE_API_KEY="sua_chave_aqui"`
- âŒ **Errado**: `OPENAI_API_KEY="sua_chave_aqui"` (isso Ã© para OpenAI/ChatGPT)

**VerificaÃ§Ãµes:**

- [ ] A variÃ¡vel no arquivo `.env` estÃ¡ como `GOOGLE_API_KEY`?
- [ ] A funÃ§Ã£o `load_dotenv()` foi chamada no inÃ­cio do script?
- [ ] A chave foi obtida em https://aistudio.google.com/app/apikey?

**CÃ³digo de verificaÃ§Ã£o:**

```python
import os
from dotenv import load_dotenv

load_dotenv()
api_key = os.getenv('GOOGLE_API_KEY')
print(f"API Key carregada: {api_key[:10]}..." if api_key else "âŒ API Key nÃ£o encontrada!")
```

### ğŸ”— 2. ConsistÃªncia das Chaves no Template

**VerificaÃ§Ãµes crÃ­ticas:**

Os nomes das variÃ¡veis no `ChatPromptTemplate` devem corresponder exatamente aos parÃ¢metros do `RunnableWithMessageHistory`:

```python
# âœ… CORRETO - As chaves devem ser IDÃŠNTICAS
template = ChatPromptTemplate.from_messages([
    ("system", "..."),
    ("placeholder", "{historico}"),    # Nome: "historico"
    ("human", "{query}")               # Nome: "query"
])

cadeia_com_memoria = RunnableWithMessageHistory(
    runnable=chain,
    get_session_history=obter_historico_por_sessao,
    input_messages_key="query",        # Mesmo nome: "query"
    history_messages_key="historico"   # Mesmo nome: "historico"
)
```

**Checklist:**

- [ ] O placeholder no template estÃ¡ como `{historico}`?
- [ ] O `history_messages_key` estÃ¡ como `"historico"`?
- [ ] A mensagem human no template estÃ¡ como `{query}`?
- [ ] O `input_messages_key` estÃ¡ como `"query"`?

### ğŸ“ 3. Estrutura Correta do invoke()

**Estrutura obrigatÃ³ria para cadeia com memÃ³ria:**

```python
# âœ… CORRETO
resposta = cadeia_com_memoria.invoke(
    {"query": pergunta},                                    # DicionÃ¡rio com a pergunta
    config={"configurable": {"session_id": "sessao_01"}}   # Config com session_id
)

# âŒ ERRADO - Sem config
resposta = cadeia_com_memoria.invoke({"query": pergunta})

# âŒ ERRADO - Config mal estruturado
resposta = cadeia_com_memoria.invoke({"query": pergunta}, {"session_id": "sessao_01"})
```

**Checklist:**

- [ ] O `invoke()` recebe dois argumentos?
- [ ] O primeiro argumento Ã© um dicionÃ¡rio com a chave `"query"`?
- [ ] O segundo argumento Ã© `config={"configurable": {"session_id": "..."}}`?
- [ ] O `session_id` estÃ¡ definido como string?

### ğŸ§  4. ValidaÃ§Ã£o da MemÃ³ria

**Teste prÃ¡tico para confirmar que a memÃ³ria estÃ¡ funcionando:**

Execute as duas perguntas sequenciais e observe:

**Pergunta 1:**

```
"Eu sou geofÃ­sico e quero migrar para a Ã¡rea de dados.
Qual linguagem de programaÃ§Ã£o devo aprender primeiro?"
```

**Resposta esperada:** O chatbot recomenda **Python** com justificativas.

**Pergunta 2:**

```
"E que tipo de projeto de portfÃ³lio eu poderia criar usando essa linguagem?"
```

**âœ… MemÃ³ria FUNCIONANDO** se:

- O chatbot menciona projetos especÃ­ficos de **Python**
- NÃ£o pergunta "qual linguagem?"
- Sugere projetos relacionados a geofÃ­sica + Python
- Demonstra continuidade da conversa

**âŒ MemÃ³ria NÃƒO FUNCIONANDO** se:

- O chatbot pergunta qual linguagem vocÃª quer usar
- DÃ¡ sugestÃµes genÃ©ricas sem mencionar Python
- NÃ£o hÃ¡ continuidade lÃ³gica entre as respostas

### ğŸ” 5. Debug do HistÃ³rico

**Script para visualizar o histÃ³rico:**

```python
# Adicione ao final do seu cÃ³digo para debug
historico = obter_historico_por_sessao("sessao_geocientista_01")
print(f"\nğŸ“Š Total de mensagens no histÃ³rico: {len(historico.messages)}")
for i, msg in enumerate(historico.messages, 1):
    tipo = "ğŸ‘¤" if msg.type == "human" else "ğŸ¤–"
    print(f"{tipo} Mensagem {i}: {msg.content[:80]}...")
```

**SaÃ­da esperada para 2 perguntas:**

```
ğŸ“Š Total de mensagens no histÃ³rico: 4
ğŸ‘¤ Mensagem 1: Eu sou geofÃ­sico e quero migrar...
ğŸ¤– Mensagem 2: OlÃ¡! Que excelente escolha!...
ğŸ‘¤ Mensagem 3: E que tipo de projeto de portfÃ³lio...
ğŸ¤– Mensagem 4: Excelente pergunta! Projetos de portfÃ³lio...
```

### ğŸ¯ 6. Checklist Final

Antes de considerar o projeto concluÃ­do, verifique:

**Arquivos:**

- [ ] `.env` existe e contÃ©m `GOOGLE_API_KEY`
- [ ] `chatbot_mentor.py` estÃ¡ com o cÃ³digo da Etapa #3
- [ ] `requirements.txt` contÃ©m todas as dependÃªncias
- [ ] `README.md` tem instruÃ§Ãµes claras

**CÃ³digo:**

- [ ] Todas as importaÃ§Ãµes estÃ£o corretas
- [ ] `load_dotenv()` Ã© chamado antes de usar a API
- [ ] Template tem system, placeholder e human
- [ ] Chain usa LCEL: `template | modelo | StrOutputParser()`
- [ ] FunÃ§Ã£o `obter_historico_por_sessao()` implementada
- [ ] `RunnableWithMessageHistory` configurado corretamente
- [ ] Loop usa `cadeia_com_memoria.invoke()` com config

**Funcionalidade:**

- [ ] CÃ³digo executa sem erros
- [ ] Primeira pergunta recebe resposta adequada
- [ ] Segunda pergunta mantÃ©m contexto da primeira
- [ ] HistÃ³rico Ã© armazenado corretamente
- [ ] MÃºltiplas sessÃµes podem coexistir

### ğŸ› Erros Comuns e SoluÃ§Ãµes

| Erro                                              | Causa                  | SoluÃ§Ã£o                                              |
| ------------------------------------------------- | ---------------------- | ---------------------------------------------------- |
| `404 models/gemini-pro is not found`              | Modelo nÃ£o disponÃ­vel  | Use `gemini-2.0-flash`                               |
| `cannot import name 'InMemoryChatMessageHistory'` | Import incorreto       | Import de `langchain_core.chat_history`              |
| `KeyError: 'query'`                               | Nome inconsistente     | Verifique que todos os `"query"` sÃ£o idÃªnticos       |
| `Session not found`                               | Config mal estruturado | Use `config={"configurable": {"session_id": "..."}}` |
| Contexto nÃ£o mantido                              | HistÃ³rico vazio        | Verifique se estÃ¡ usando `cadeia_com_memoria`        |

---

## ğŸ“„ LicenÃ§a

Este projeto Ã© de cÃ³digo aberto e estÃ¡ disponÃ­vel para fins educacionais.

---

**ğŸ‰ ParabÃ©ns por concluir o projeto GeoAI Mentor! ğŸ‰**

Se seguiu todos os pontos de revisÃ£o acima, seu chatbot estÃ¡ funcionando perfeitamente com memÃ³ria conversacional completa! ğŸš€
