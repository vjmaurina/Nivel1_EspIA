import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Instancia o modelo Gemini
modelo = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    temperature=0.7
)

# Cria o template de prompt com personalidade do GeoAI Mentor
template = ChatPromptTemplate.from_messages([
    ("system", "Voc√™ √© o 'GeoAI Mentor', um assistente especializado em ajudar geocientistas a migrar para a √°rea de Ci√™ncia de Dados. Seja amig√°vel e did√°tico."),
    ("placeholder", "{historico}"),
    ("human", "{query}")
])

# Cria a cadeia (chain) usando LCEL
chain = template | modelo | StrOutputParser()

# Dicion√°rio para armazenar o hist√≥rico de cada sess√£o
memoria_sessoes = {}

# Fun√ß√£o para obter ou criar hist√≥rico por sess√£o (padr√£o singleton)
def obter_historico_por_sessao(session_id: str):
    if session_id not in memoria_sessoes:
        memoria_sessoes[session_id] = InMemoryChatMessageHistory()
    return memoria_sessoes[session_id]

# Cria a cadeia com mem√≥ria
cadeia_com_memoria = RunnableWithMessageHistory(
    runnable=chain,
    get_session_history=obter_historico_por_sessao,
    input_messages_key="query",
    history_messages_key="historico"
)

# Lista de perguntas sequenciais
perguntas = [
    "Eu sou geof√≠sico e quero migrar para a √°rea de dados. Qual linguagem de programa√ß√£o devo aprender primeiro?",
    "E que tipo de projeto de portf√≥lio eu poderia criar usando essa linguagem?"
]

# Configura√ß√£o da sess√£o
config = {"configurable": {"session_id": "sessao_geocientista_01"}}

# Loop que envia cada pergunta para o modelo e imprime a resposta
for pergunta in perguntas:
    print(f"\nüîµ Pergunta: {pergunta}")
    # Usa a cadeia com mem√≥ria
    resposta = cadeia_com_memoria.invoke({"query": pergunta}, config=config)
    print(f"\nü§ñ GeoAI Mentor: {resposta}")
    print("-" * 80)

# Exibe o hist√≥rico completo da conversa
print("\n" + "=" * 80)
print("üìù HIST√ìRICO DA CONVERSA:")
print("=" * 80)
historico = obter_historico_por_sessao("sessao_geocientista_01")
for mensagem in historico.messages:
    tipo = "üë§ Usu√°rio" if mensagem.type == "human" else "ü§ñ GeoAI Mentor"
    print(f"\n{tipo}: {mensagem.content[:100]}...")
print("=" * 80)
